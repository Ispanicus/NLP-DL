{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - NLP and Deep Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 - Language Identification with a Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, you will implement the forward step of a FFNN from scratch and compare your solution to Pytorch on a small toy example to predict the language for a given word. \n",
    "\n",
    "It is very important that you understand the basic building blocks (input/output: how to encode your instances, the labels; the model: what the neural network consists of, how to learn its weights, how to do a forward pass for prediction). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Representing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming multi-class classification tasks for the assignments of this week. The labels are: $$ y \\in \\{da,nl,en\\}$$\n",
    "\n",
    "We will use the same data as in week2, from:\n",
    "* English [Wookipedia](https://starwars.fandom.com/wiki/Main_Page)  \n",
    "* Danish [Kraftens Arkiver](https://starwars.fandom.com/da/wiki) \n",
    "* Dutch [Yodapedia](https://starwars.fandom.com/da/wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_langid(path):\n",
    "    text = []\n",
    "    labels = []\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        tok = line.strip().split('\\t')\n",
    "        labels.append(tok[0])\n",
    "        text.append(tok[1])\n",
    "    return text, labels\n",
    "\n",
    "wooki_train_text, wooki_train_labels = load_langid(f'langid-data/wookipedia_langid.train.tok.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a): Convert the training data into n-hot format, where each feature represents whether a **single character** is present or not.  Similarly, convert the labels into numeric format. For simplicity, you can assume a closed vocabulary (only the letters in wookie_train_text, no unknown-character handling). Keep original casing, and assign the character indices based on their chronological order.\n",
    "\n",
    "  * What is the vocabulary size?\n",
    "  \n",
    "**Hint:** It is easier for the rest of the assignment if you directly use a torch tensor to save the features ([tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py), another [introduction](https://towardsdatascience.com/an-easy-introduction-to-pytorch-for-neural-networks-3ea08516bff2)), a 2d torch tensor filled with 0's can be initiated with: `torch.zeros(dim1, dim2, dtype=float)`. Note the use of `float` instead of `int` here, which is only because the `torch.mm` requires float tensors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 131\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "all_text_combined = ''.join(wooki_train_text)\n",
    "first_occurrence = {}\n",
    "for char in all_text_combined:\n",
    "    if char not in first_occurrence:\n",
    "        first_occurrence[char] = len(first_occurrence)\n",
    "\n",
    "# Now first_occurrence contains characters as keys and their chronological indices as values.\n",
    "\n",
    "# Initialize the feature tensor with zeros, using the efficient mapping\n",
    "features = torch.zeros((len(wooki_train_text), len(first_occurrence)), dtype=torch.float)\n",
    "\n",
    "# Fill the feature tensor with 1s where the character is present in the sentence\n",
    "for i, sentence in enumerate(wooki_train_text):\n",
    "    for char in set(sentence): # We use set to avoid duplicate character counting\n",
    "        features[i, first_occurrence[char]] = 1\n",
    "\n",
    "label_to_idx = {'da': 0, 'nl': 1, 'en': 2}\n",
    "labels = torch.tensor([label_to_idx[label] for label in wooki_train_labels], dtype=torch.float)\n",
    "\n",
    "print(\"Vocab size:\", len(first_occurrence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2: Forward pass (from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Networks (FNNs) or MLPs\n",
    "\n",
    "Feedforward Neural Networks (FNNs) are also called Multilayer Perceptrons (MLPs). These are the most basic types of neural networks. They are called this way as the information is flowing from the input nodes through the network up to the output nodes. \n",
    "\n",
    "It is essential to understand that a neural network is a non-linear classification model which is based upon function application. Each layer in a neural network is an application of a function.\n",
    "\n",
    "Summary (by J.Frellsen):\n",
    "<img src=\"pics/fnn_jf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to implement the forward step manually on a small dataset. You will create a network following the design in the following figure (note that the input should be the sames size as the number of characters found in the previous assignment, instead of 4):\n",
    "\n",
    "<img src=\"pics/nn.svg\">\n",
    "\n",
    "a) How many neurons do hidden layer 1 and hidden layer 2 have? Note: the bias node is not shown in the figure, you do not have to count them for this assignment.\n",
    "\n",
    "b) How many neurons does the output layer have? And the input layer? (Note: the figure shows only 4 input nodes, in this example your input size is defined in the previous assignment - what is the input layer size?)\n",
    "\n",
    "c) Specify the size of layers of the feedforward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions to determine the input and output dimensions of each layer\n",
    "input_dim = features.shape[1] # 131\n",
    "\n",
    "hidden_dim1 = 15\n",
    "hidden_dim2 = 20\n",
    "\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Now initialize the layers themselves as torch tensors (do not use a torch.module here!). You can define the bias and the weights in separate tensors. The weights should be initialized randomly (`torch.randn((dim1, dim2), dtype=torch.float)`, see also [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html)) and the biases can be set to 1 (`torch.ones(dim1, dtype=torch.float)`, see also [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html)). Confirm whether their size match the answer to `b)` and `a)` by printing .shape of the tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: torch.Size([15000, 131])\n",
      "hidden1: torch.Size([131, 15])\n",
      "bias1: torch.Size([1, 15])\n",
      "hidden2: torch.Size([15, 20])\n",
      "bias2: torch.Size([1, 20])\n",
      "output: torch.Size([20, 3])\n",
      "output_bias: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "## define all parameters of this NN\n",
    "\n",
    "hidden1 = torch.randn((input_dim,hidden_dim1),dtype=torch.float)\n",
    "bias1 = torch.ones((1,hidden_dim1),dtype=torch.float)\n",
    "hidden2 = torch.randn((hidden_dim1,hidden_dim2),dtype=torch.float)\n",
    "bias2 = torch.ones((1,hidden_dim2),dtype=torch.float)\n",
    "output = torch.randn((hidden_dim2,output_dim),dtype=torch.float)\n",
    "output_bias = torch.ones((1,output_dim),dtype=torch.float)\n",
    "\n",
    "# print the shapes of all parameters\n",
    "print(\"features:\", features.shape)\n",
    "print(\"hidden1:\", hidden1.shape)\n",
    "print(\"bias1:\", bias1.shape)\n",
    "print(\"hidden2:\", hidden2.shape)\n",
    "print(\"bias2:\", bias2.shape)\n",
    "print(\"output:\", output.shape)\n",
    "print(\"output_bias:\", output_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the shape of all parameters, we are ready to \"connect the dots\" and build the network. \n",
    "\n",
    "It is instructive to break the computation of each layer down into two steps: the scores $a1$ are obtained by the linear function followed by the activation applications $\\sigma$ to obtain the representation $z1$, as in:\n",
    "\n",
    "$$ a1 = xW_1 + b_1$$\n",
    "$$ z1 = \\sigma(a1)$$\n",
    "\n",
    "d) Specify the entire network up to the output layer $z3$, and **up to and exclusive** the final application of the softmax, the last activation function, which is provided. For multiplication [torch.mm](https://pytorch.org/docs/stable/generated/torch.mm.html) can be used. Use a tanh activation function: [torch.tanh](https://pytorch.org/docs/stable/generated/torch.tanh.html).\n",
    "\n",
    "The exact implementation of the softmax might differ from toolkit to toolkit (due to variations in implementation details in order to obtain numerical stability). Therefore, we will use the Pytorch implementation for the softmax calculation ([torch.nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8842, -2.7971, -0.3018],\n",
      "        [-5.1118,  2.4100, -0.4130],\n",
      "        [-3.0317, -8.0785, -0.5843],\n",
      "        ...,\n",
      "        [-3.1447, -1.1899, -0.4766],\n",
      "        [-4.2456, -0.6283, -0.2588],\n",
      "        [-2.1948, -0.1600, -0.0308]])\n"
     ]
    }
   ],
   "source": [
    "## implement the forward pass (up to and exclusive the softmax) \n",
    "## apply it to the training data `data_train` - use vectorization\n",
    "\n",
    "z1 = torch.tanh(torch.mm(features,hidden1) + bias1)\n",
    "z2 = torch.tanh(torch.mm(z1,hidden2) + bias2)\n",
    "z3 = torch.mm(z2,output) + output_bias\n",
    "\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x):\n",
    "    input_dim = x.shape[1]\n",
    "    hidden_dim1 = 15\n",
    "    hidden_dim2 = 20\n",
    "    output_dim = 3\n",
    "    hidden1 = torch.randn((input_dim,hidden_dim1),dtype=torch.float)\n",
    "    bias1 = torch.ones((1,hidden_dim1),dtype=torch.float)\n",
    "    hidden2 = torch.randn((hidden_dim1,hidden_dim2),dtype=torch.float)\n",
    "    bias2 = torch.ones((1,hidden_dim2),dtype=torch.float)\n",
    "    output = torch.randn((hidden_dim2,output_dim),dtype=torch.float)\n",
    "    output_bias = torch.ones((1,output_dim),dtype=torch.float)\n",
    "    z1 = torch.tanh(torch.mm(x,hidden1) + bias1)\n",
    "    z2 = torch.tanh(torch.mm(z1,hidden2) + bias2)\n",
    "    z3 = torch.mm(z2,output) + output_bias\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    return softmax(z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that all predictions sum up to approximately 1 (hint: use `torch.sum` with `axis=1`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(forward_pass(features), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Congrats! you have made it through the manual construction of the forward pass. Note that these weights are still random, so performance is not expected to be good. Now lets compare your implementation to a set of pre-determined weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Where do the weights come from?  Loading existing weights\n",
    "\n",
    "So far, the model that you used randomly initialized weights. In this step we will load pre-trained model weights and do the forward pass with those weights, in order to check your implementation against model predictions computed by the toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to:\n",
    "* load pretrained weights for all parameters\n",
    "* apply the weights to the evaluation data\n",
    "* check that your manual softmax scores match the ones obtained by the pre-trained model `model` that we will load\n",
    "* convert the output to labels and calculate the accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# use the character indexing from assignment 3\n",
    "idx2char = ['H', 'e', ' ', 'v', 'n', 'w', 't', 's', 'o', 'f', 'a', 'r', 'u', 'g', 'h', ',', 'i', 'c', 'y', 'd', 'b', 'm', 'p', 'l', 'k', '.', 'D', 'E', 'C', 'j', 'R', 'S', 'U', '1', \"'\", 'æ', 'å', 'q', '`', 'I', '(', ')', 'M', 'F', '-', 'x', 'K', '9', '5', 'B', 'W', 'z', 'G', 'P', 'L', '/', 'O', '6', 'T', '7', 'Z', '2', '0', 'J', 'V', 'A', 'ø', 'X', '–', 'N', 'ë', ':', '&', '3', 'Y', 'é', '4', '[', ']', '’', ';', '8', 'É', 'Æ', 'Q', '!', '—', 'ï', '°', 'ō', '\\u200b', '‘', 'ń', '“', '”', '?', 'Å', '<', '>', '#', '%', '+', 'ʊ', 'ɹ', 'ə', 'ɑ', 'ö', 'à', 'á', 'è', '=', 'ü', 'Ø', '∑', '^', 'ś', 'ñ', '|', '½', '$', '«', '™', 'ó', '´', '…', '―', '»', 'ː', 'θ', '²', 'Θ']\n",
    "char2idx = {'H': 0, 'e': 1, ' ': 2, 'v': 3, 'n': 4, 'w': 5, 't': 6, 's': 7, 'o': 8, 'f': 9, 'a': 10, 'r': 11, 'u': 12, 'g': 13, 'h': 14, ',': 15, 'i': 16, 'c': 17, 'y': 18, 'd': 19, 'b': 20, 'm': 21, 'p': 22, 'l': 23, 'k': 24, '.': 25, 'D': 26, 'E': 27, 'C': 28, 'j': 29, 'R': 30, 'S': 31, 'U': 32, '1': 33, \"'\": 34, 'æ': 35, 'å': 36, 'q': 37, '`': 38, 'I': 39, '(': 40, ')': 41, 'M': 42, 'F': 43, '-': 44, 'x': 45, 'K': 46, '9': 47, '5': 48, 'B': 49, 'W': 50, 'z': 51, 'G': 52, 'P': 53, 'L': 54, '/': 55, 'O': 56, '6': 57, 'T': 58, '7': 59, 'Z': 60, '2': 61, '0': 62, 'J': 63, 'V': 64, 'A': 65, 'ø': 66, 'X': 67, '–': 68, 'N': 69, 'ë': 70, ':': 71, '&': 72, '3': 73, 'Y': 74, 'é': 75, '4': 76, '[': 77, ']': 78, '’': 79, ';': 80, '8': 81, 'É': 82, 'Æ': 83, 'Q': 84, '!': 85, '—': 86, 'ï': 87, '°': 88, 'ō': 89, '\\u200b': 90, '‘': 91, 'ń': 92, '“': 93, '”': 94, '?': 95, 'Å': 96, '<': 97, '>': 98, '#': 99, '%': 100, '+': 101, 'ʊ': 102, 'ɹ': 103, 'ə': 104, 'ɑ': 105, 'ö': 106, 'à': 107, 'á': 108, 'è': 109, '=': 110, 'ü': 111, 'Ø': 112, '∑': 113, '^': 114, 'ś': 115, 'ñ': 116, '|': 117, '½': 118, '$': 119, '«': 120, '™': 121, 'ó': 122, '´': 123, '…': 124, '―': 125, '»': 126, 'ː': 127, 'θ': 128, '²': 129, 'Θ': 130}\n",
    "\n",
    "# the label indexes that were used during training\n",
    "label2idx = {'da':0, 'nl':1, 'en':2}\n",
    "idx2label = ['da', 'nl', 'en']\n",
    "\n",
    "# This is the definition of an FNN model in PyTorch, and can mostly be ignored for now.\n",
    "# We will focus on how to create Torch models in week 5\n",
    "class LangId(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.input = nn.Linear(vocab_size, 15)\n",
    "        self.hidden1 = nn.Linear(15, 20)\n",
    "        self.hidden2 = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.input(x))\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = self.hidden2(x)\n",
    "        return x\n",
    "\n",
    "lang_classifier = torch.load(f'model.th')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the weights you just loaded using the `state_dict()` function of the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input.weight',\n",
       "              tensor([[ 0.1274,  0.2723,  0.4691,  ...,  0.0754,  0.0201,  0.0813],\n",
       "                      [-0.1876,  0.3465,  0.4979,  ..., -0.0436, -0.0362, -0.0866],\n",
       "                      [ 0.1779,  0.3311,  0.3578,  ..., -0.0705,  0.0656,  0.0415],\n",
       "                      ...,\n",
       "                      [-0.0264,  0.2019,  0.1753,  ...,  0.0335,  0.0764,  0.0222],\n",
       "                      [-0.0810, -0.3535, -0.1255,  ..., -0.0645,  0.0299,  0.0438],\n",
       "                      [ 0.0740, -0.1535,  0.1290,  ..., -0.0464, -0.0612,  0.0650]])),\n",
       "             ('input.bias',\n",
       "              tensor([ 0.4091,  0.8057,  0.4696,  0.3282,  0.4459, -0.3094, -0.7575, -0.3531,\n",
       "                      -0.3175,  0.2946,  0.7420,  0.1358,  0.1037, -0.2193, -0.3283])),\n",
       "             ('hidden1.weight',\n",
       "              tensor([[ 0.2575,  0.6953,  0.5631,  0.2704, -0.3716, -0.9438, -0.4709, -0.9932,\n",
       "                       -0.7564, -0.0925, -2.2822,  0.2297,  0.2956,  0.0241, -1.9843],\n",
       "                      [ 0.1413,  0.2127,  0.4845, -0.0701,  0.6703, -0.3185, -0.3957,  0.1315,\n",
       "                       -0.6918, -0.2745, -0.1002,  0.2550,  0.4770,  0.1109, -0.3992],\n",
       "                      [ 0.7823,  0.1207,  0.3213, -0.0475,  0.7223,  0.1428, -0.5531,  0.2000,\n",
       "                        0.0893, -0.3841,  0.1112, -0.0569,  0.5947, -0.6937,  0.4243],\n",
       "                      [ 0.0985,  1.5386, -0.3472, -0.8660,  0.7426, -0.0289, -0.0979, -0.2248,\n",
       "                       -0.0257,  0.4956, -1.0115, -0.2432, -0.4809,  0.6184,  0.6953],\n",
       "                      [ 0.0232, -0.2676, -0.0068,  0.3410, -0.4943,  0.6073,  0.1574,  0.6130,\n",
       "                        0.8132, -0.0182,  1.0047, -0.2493, -0.1170, -0.4713,  0.8951],\n",
       "                      [-0.4531,  0.0077,  0.1982, -0.2570,  0.9576,  1.5319,  1.0911, -0.1294,\n",
       "                       -0.1513,  0.4058,  0.8775, -1.4542, -0.6756,  0.6437, -0.0637],\n",
       "                      [-0.0188,  0.1584, -0.2749,  0.1247,  0.4504,  0.6595,  0.4859, -0.0764,\n",
       "                        0.3448,  0.2107,  0.3694, -0.5843, -0.0219,  0.1169,  0.4314],\n",
       "                      [ 0.2663,  0.4091,  0.4482, -0.0810,  0.6472, -0.0926, -0.1696, -0.2151,\n",
       "                       -0.6046, -0.2196, -0.0106,  0.0498,  0.3613,  0.1652, -0.0080],\n",
       "                      [ 0.0238, -0.7592,  0.3622,  0.2082, -0.1043,  0.3239, -0.0603,  0.2656,\n",
       "                        0.3006, -0.2938,  0.5831, -0.0507, -0.0260, -0.3097,  0.2777],\n",
       "                      [ 0.2607,  0.9150, -0.2169, -0.1303,  0.5222, -0.0239, -0.6132, -0.1086,\n",
       "                       -0.1612,  0.0567,  0.0154,  0.2324, -0.5132,  0.8815,  0.2304],\n",
       "                      [ 0.0487, -0.4816, -0.1768, -0.0849, -0.2191, -0.1497, -0.2482,  0.3167,\n",
       "                        1.2500, -0.0845,  0.8245,  0.4551,  0.0036, -0.5018,  0.0255],\n",
       "                      [-0.0802,  0.5033, -0.0674,  0.0871,  0.2249,  0.4973,  0.3259,  0.4636,\n",
       "                        0.7863, -0.0290,  1.0510, -0.2961, -0.1969,  0.5019,  1.9955],\n",
       "                      [ 0.0207,  0.1539, -0.0628,  0.0138,  0.8202,  0.4971,  0.8012,  0.0564,\n",
       "                       -0.1323, -0.0718, -0.2645, -1.0286, -0.1922,  0.3575,  0.3256],\n",
       "                      [ 0.6240,  0.3164, -0.1598, -0.6158,  0.5465,  0.0340,  0.0693,  0.2907,\n",
       "                        0.7922,  0.1091, -0.0147, -0.6633,  0.0804, -0.1143,  0.5061],\n",
       "                      [ 0.1215,  0.4285,  0.2948, -0.9482,  0.0128, -0.8172, -0.5179, -0.7863,\n",
       "                       -0.8570,  0.0745, -0.5490,  0.6721,  0.2755, -0.2263, -0.3991],\n",
       "                      [-1.0201,  0.0455, -0.9147, -0.7720, -0.4775,  0.5190,  0.5954,  0.3022,\n",
       "                        0.0520,  0.4095,  0.3205, -0.5907, -0.5462, -0.1718,  1.2266],\n",
       "                      [-0.1344, -0.1653, -0.2833, -0.3142, -0.1152,  0.2869,  0.2829,  0.0184,\n",
       "                        0.1363,  0.3274, -0.0947, -0.3415, -0.1789,  0.0378,  0.0306],\n",
       "                      [-0.1025,  0.6285, -0.0914, -0.1482,  0.0396, -0.4274,  0.1675, -0.2574,\n",
       "                       -0.0724, -0.0152,  0.2160, -0.1630,  0.1252,  0.1318,  0.1642],\n",
       "                      [ 0.5360,  0.7464,  0.3229, -0.2490,  0.6192, -0.1805, -0.1130,  0.2933,\n",
       "                       -0.0098,  0.0768,  1.5201, -0.4075,  0.0207,  0.5212,  1.6912],\n",
       "                      [ 0.5786, -0.6360,  0.2668,  0.7761, -0.6159, -1.0443, -0.5363, -0.5601,\n",
       "                       -0.9773,  0.1997, -2.5120,  0.8936,  0.7410,  0.4194, -1.9675]])),\n",
       "             ('hidden1.bias',\n",
       "              tensor([ 0.6749,  0.1967,  0.3827, -0.8974, -0.0010, -0.6915,  0.1325,  0.2587,\n",
       "                      -0.1166, -0.4284,  0.2775,  0.1698, -0.1733, -0.5159, -0.1306, -0.6334,\n",
       "                      -0.0710, -0.0774,  0.4660,  0.7498])),\n",
       "             ('hidden2.weight',\n",
       "              tensor([[-1.1929,  0.7590, -0.4203,  0.6335, -0.5118, -0.6470,  0.1136,  0.7656,\n",
       "                       -0.2115,  0.4258, -0.1070,  1.3044, -0.3927,  0.4815, -0.2065,  0.1193,\n",
       "                        0.5277, -0.0628,  1.0586, -1.2823],\n",
       "                      [-0.1084, -0.3936,  0.2931, -0.8568,  0.0422,  0.2233, -0.1856, -0.4776,\n",
       "                        0.3987, -0.6149,  0.4459, -0.1198,  0.2158, -0.1368, -0.3301, -0.5805,\n",
       "                        0.1345, -0.3114, -0.2366,  0.5852],\n",
       "                      [ 1.2269, -0.5441, -0.0556,  0.6234,  0.3867,  0.3451,  0.2686, -0.6395,\n",
       "                       -0.5229,  0.2870, -0.3788, -1.0749,  0.3720, -0.4522,  0.4057,  0.1022,\n",
       "                       -0.1416, -0.1017, -0.5362,  0.9197]])),\n",
       "             ('hidden2.bias', tensor([ 0.6436,  0.2206, -0.8653]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_classifier.state_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Convert the following dev data into the input format for the neural network above. \n",
    "\n",
    "**Hint** The indices of the characters are based on the order in the training data, and should match in the development data, we provide the correct idx2char and char2idx that were used to train the model in the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "wooki_dev_text, wooki_dev_labels = load_langid('langid-data/wookipedia_langid.dev.tok.txt')\n",
    "\n",
    "# Initialize the feature tensor with zeros, using the efficient mapping\n",
    "dev_features = torch.zeros((len(wooki_dev_text), len(char2idx)), dtype=torch.float)\n",
    "\n",
    "# Fill the feature tensor with 1s where the character is present in the sentence\n",
    "for i, sentence in enumerate(wooki_dev_text):\n",
    "    for char in set(sentence): # We use set to avoid duplicate character counting\n",
    "        dev_features[i, char2idx[char]] = 1\n",
    "\n",
    "dev_labels = torch.tensor([label_to_idx[label] for label in wooki_dev_labels], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) run a forward pass on the dev-data with `lang_classifier`, using the forward() function\n",
    "\n",
    "* c) Apply your manual implementation of the forward pass to the evaluation data by using the parameters (weights) you just loaded with `state_dict()`. This allows you to check if you get the same results back as the model implemented in Torch. If the outputs match, you implemented the forward pass correctly, congratulations!\n",
    "\n",
    "**Hint**: internally the torch model saves the weight in a transposed vector for efficiency reasons. This means that W1 will have the dimension of (15,131). To use your previous implementation you have to call the the transpose function in Pytorch ([`.t()`](https://pytorch.org/docs/stable/generated/torch.t.html)), which will convert the shape to be (131,15)\n",
    "\n",
    "* d) Now apply softmax on the resulting weights and convert the output to the label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3604,  0.7146,  0.3900],\n",
       "        [-2.2557,  1.4831, -0.5462],\n",
       "        [-2.0805,  1.3734, -0.5807],\n",
       "        ...,\n",
       "        [ 2.0050, -1.2380, -1.6113],\n",
       "        [ 2.2639, -1.3041, -1.8159],\n",
       "        [-1.7680, -1.5234,  2.6722]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b)\n",
    "\n",
    "lang_classifier.forward(dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['input.weight', 'input.bias', 'hidden1.weight', 'hidden1.bias', 'hidden2.weight', 'hidden2.bias'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_classifier.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3604,  0.7146,  0.3900],\n",
      "        [-2.2557,  1.4831, -0.5462],\n",
      "        [-2.0805,  1.3734, -0.5807],\n",
      "        ...,\n",
      "        [ 2.0050, -1.2380, -1.6113],\n",
      "        [ 2.2639, -1.3041, -1.8159],\n",
      "        [-1.7680, -1.5234,  2.6722]])\n"
     ]
    }
   ],
   "source": [
    "# starting point for c)\n",
    "W1 = lang_classifier.state_dict()['input.weight'].t()\n",
    "B1 = lang_classifier.state_dict()['input.bias'].t()\n",
    "W2 = lang_classifier.state_dict()['hidden1.weight'].t()\n",
    "B2 = lang_classifier.state_dict()['hidden1.bias'].t()\n",
    "W3 = lang_classifier.state_dict()['hidden2.weight'].t()\n",
    "B3 = lang_classifier.state_dict()['hidden2.bias'].t()\n",
    "\n",
    "z1 = torch.tanh(torch.mm(dev_features,W1) + B1)\n",
    "z2 = torch.tanh(torch.mm(z1,W2) + B2) \n",
    "z3 = torch.mm(z2,W3) + B3\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8136666417121887\n"
     ]
    }
   ],
   "source": [
    "# d)\n",
    "softmax = torch.nn.Softmax(dim=1) \n",
    "preds = [torch.argmax(x) for x in softmax(z3)]\n",
    "print(\"Accuracy:\",(sum(a == b for a, b in zip(preds, dev_labels))/len(preds)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: What do word embeddings represent?\n",
    "In the following exercises, you are going to explore what is represented with word embeddings. You are going to make use of the python gensim package and two sets of pre-trained embeddings. The embeddings can be downloaded from:\n",
    "\n",
    "* http://itu.dk/people/robv/data/embeds/twitter.bin.tar.gz\n",
    "* http://itu.dk/people/robv/data/embeds/GoogleNews-50k.bin.tar.gz\n",
    "\n",
    "The first embeddings are skip-gram embeddings trained on a collection of 2 billion words from English tweets collected during 2012 and 2018 with the default settings of word2vec. The second embeddings are trained on 100 billion words from Google News. They have both been truncated to the most frequent 500,000 words. Note that loading that each of these embeddings require approximately 2GB of ram.\n",
    "\n",
    "The embeddings can be loaded in gensim as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-50k.bin\n"
     ]
    }
   ],
   "source": [
    "#!tar xvzf twitter.bin.tar.gz\n",
    "#!tar xvzf GoogleNews-50k.bin.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading finished\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "twitEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                                'twitter.bin', binary=True)\n",
    "googEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                                'GoogleNews-50k.bin', binary=True)\n",
    "print('loading finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the index operator ``[]`` or the function ``get_vector()`` to acces the individual word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.64285821e-01,  2.37979457e-01, -4.24226150e-02, -4.35831666e-01,\n",
       "       -4.06450212e-01, -1.43117514e-02,  1.22334510e-01, -5.59092343e-01,\n",
       "        1.23332568e-01,  2.36625358e-01,  3.58797014e-02, -9.40739065e-02,\n",
       "       -2.04128489e-01, -1.81295779e-02, -1.08792759e-01, -2.70818472e-01,\n",
       "        1.05479717e-01,  1.37095019e-01,  1.79271579e-01,  2.91243941e-01,\n",
       "       -5.87746739e-01,  2.90462654e-02,  6.89281642e-01, -1.80917114e-01,\n",
       "       -2.57750720e-01, -2.01395631e-01, -5.16403615e-01,  5.85804135e-03,\n",
       "       -1.67768478e-01,  2.17095211e-01,  2.22494245e-01,  1.56742647e-01,\n",
       "       -3.60864878e-01,  3.94283593e-01,  8.04448500e-03,  1.11518592e-01,\n",
       "       -1.85592070e-01, -1.16088443e-01,  3.24357510e-01,  4.00876179e-02,\n",
       "        9.14092362e-02, -1.04118213e-01, -6.89513862e-01,  1.54412836e-01,\n",
       "        4.57625002e-01,  2.55037360e-02, -3.84058757e-03,  7.12698698e-02,\n",
       "       -2.25590184e-01, -1.96693689e-01, -3.88458431e-01, -2.27625713e-01,\n",
       "        6.94357634e-01, -3.22451681e-01,  1.02136515e-01, -2.06018016e-01,\n",
       "        4.12042558e-01, -5.69718063e-01, -1.77221447e-01, -7.04838037e-01,\n",
       "        5.86289287e-01,  1.18259907e-01, -5.15342169e-02,  3.12465429e-01,\n",
       "       -5.25288224e-01,  5.48078716e-01,  2.75395304e-01, -1.61753371e-01,\n",
       "        4.37383980e-01, -9.72139016e-02, -1.71533942e-01,  3.94486845e-01,\n",
       "        1.33596465e-01,  3.94779667e-02,  1.23597078e-01,  3.22522134e-01,\n",
       "       -1.40469015e-01, -7.82357603e-02, -3.39861751e-01, -4.84348953e-01,\n",
       "        8.03721175e-02,  1.13537483e-01, -6.08491674e-02,  2.59142101e-01,\n",
       "        3.79286081e-01, -3.21717769e-01,  3.45237699e-04,  4.53131020e-01,\n",
       "        2.97795296e-01,  4.74226564e-01, -4.53676343e-01,  3.24836336e-02,\n",
       "       -3.32390517e-01, -2.07979798e-01, -1.37789533e-01, -1.56768903e-01,\n",
       "       -4.80998158e-02,  1.80168718e-01,  6.78501278e-03, -9.98419821e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitEmbs['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word similarities\n",
    "Cosine distance can be used to measure the distance between two words. It is defined as:\n",
    "\\begin{equation}\n",
    "cos_{\\vec{a},\\vec{b}} = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}| |\\vec{b}|} = \\frac{\\sum^n_1 a_i b_i}{\\sqrt{\\sum^n_1 a_i^2} \\sqrt{\\sum^n_1 b_i^2}}\n",
    "\\end{equation}\n",
    "\n",
    "* a) Implement the cosine similarity using pure python (only the ``math`` package is allowed). Note that `similarity == 1-distance`.\n",
    "\n",
    "You can compare your scores to the gensim implementation to check wheter it is correct. The following code should give the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1044651\n",
      "0.1044651\n"
     ]
    }
   ],
   "source": [
    "#a)\n",
    "import math\n",
    "def cosine(vec1, vec2):\n",
    "    magnitude_vec1 = math.sqrt(sum(a**2 for a in vec1))\n",
    "    magnitude_vec2 = math.sqrt(sum(b**2 for b in vec2))\n",
    "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
    "        # If either vector has zero magnitude, similarity is not defined\n",
    "        return 0\n",
    "    else:\n",
    "        dot_product = sum(a*b for a, b in zip(vec1, vec2))\n",
    "        similarity = 1 - (dot_product/(magnitude_vec1 * magnitude_vec2))\n",
    "        return similarity\n",
    "    \n",
    "print(\"{:.7f}\".format(twitEmbs.distance('cat', 'dog')))\n",
    "print(\"{:.7f}\".format(cosine(twitEmbs['cat'], twitEmbs['dog'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In wordnet, the distance between two senses can be based on the distance in the taxonomy. The most common metric for this is:\n",
    "\n",
    "* Wu-Palmer Similarity: denotes how similar two word senses are, based on the depth of the two senses in the taxonomy and of their Least Common Subsumer (most specific ancestor node).\n",
    "\n",
    "It can be obtained in python like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/alanispani/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet similarity: 0.8571428571428571\n",
      "Twitter similarity: 0.8955349\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "first_word = wordnet.synsets('cat')[0] #0 means: most common sense\n",
    "second_word = wordnet.synsets('dog')[0]\n",
    "print('WordNet similarity: ' + str(first_word.wup_similarity(second_word)))\n",
    "\n",
    "print('Twitter similarity: ' + str(twitEmbs.similarity('cat', 'dog')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* b) Think of 5 word pairs which have a high similarity according to you. Estimate the difference between these pairs in wordnet as well as in the Twitter embeddings and the Google News embeddings. Which method is closest to your own intuition? (You are allowed to use the gensim implementation of cosine similarity here.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet similarity of Joy to Happiness: 0.8\n",
      "Twitter similarity of Joy to Happiness: 0.5528852\n",
      "Google similarity of Joy to Happiness: 0.27504772\n",
      "WordNet similarity of Quick to Fast: 0.11764705882352941\n",
      "Twitter similarity of Quick to Fast: 0.6936942\n",
      "Google similarity of Quick to Fast: 0.43288276\n",
      "WordNet similarity of Ocean to Sea: 0.8\n",
      "Twitter similarity of Ocean to Sea: 0.76649284\n",
      "Google similarity of Ocean to Sea: 0.50030446\n",
      "WordNet similarity of Intelligent to Smart: 0.16666666666666666\n",
      "Twitter similarity of Intelligent to Smart: 0.7548678\n",
      "Google similarity of Intelligent to Smart: 0.5120937\n",
      "WordNet similarity of House to Home: 0.35294117647058826\n",
      "Twitter similarity of House to Home: 0.64104354\n",
      "Google similarity of House to Home: 0.22996894\n"
     ]
    }
   ],
   "source": [
    "list1 = [\"Joy\", \"Quick\", \"Ocean\", \"Intelligent\", \"House\"]\n",
    "list2 = [\"Happiness\", \"Fast\", \"Sea\", \"Smart\", \"Home\"]\n",
    "\n",
    "for one, two in zip(list1, list2):\n",
    "    first_word = wordnet.synsets(one)[0]\n",
    "    second_word = wordnet.synsets(two)[0]\n",
    "    print(f'WordNet similarity of {one} to {two}: ' + str(first_word.wup_similarity(second_word)))\n",
    "    print(f'Twitter similarity of {one} to {two}: ' + str(twitEmbs.similarity(one, two)))\n",
    "    print(f'Google similarity of {one} to {two}: ' + str(googEmbs.similarity(one, two)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analogies\n",
    "\n",
    "Analogies have often been used to demonstrate the power of word embeddings. Analogies have the form ``A :: B : C :: D``. In this setting `A`, `B` and `C` are usually given and the fourth term `D` is extracted from the embeddings by using ``3cosadd``:\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{d}{\\mathrm{argmax}} (\\cos (d, c) - \\cos (d, a) + \\cos (d, b))\n",
    "\\end{equation}\n",
    "\n",
    "You can query analogies with gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8401796221733093),\n",
       " ('goddess', 0.7309160232543945),\n",
       " ('king…', 0.7233694195747375),\n",
       " ('princess', 0.715788722038269),\n",
       " ('kings', 0.707615852355957),\n",
       " ('godess', 0.6952609419822693),\n",
       " ('Queen', 0.6902579069137573),\n",
       " ('queen,', 0.6876209378242493),\n",
       " ('quee…', 0.68569016456604),\n",
       " ('queens', 0.6832401752471924)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Man is to king as woman is to ...?\n",
    "twitEmbs.most_similar(positive=['woman', 'king'], negative=['man'], \n",
    "                                                         topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``3cosadd`` can be used to solve semantic as well as syntactic analogies:\n",
    "\n",
    "| Semantic            |                                      |\n",
    "|---------------------|--------------------------------------|\n",
    "| Country-capital     | Denmark :: Copenhagen : England :: X |\n",
    "| Family-relations    | boy :: girl : he :: X                |\n",
    "| Object-color        | sky :: blue : grass :: X             |\n",
    "\n",
    "| Syntactic           |                                      |\n",
    "|---------------------|--------------------------------------|\n",
    "| Superlatives        | nice :: nicer : good :: X            |\n",
    "| Present-past tense  | work :: worked : drink :: X          |\n",
    "| Country-nationality | Brazil :: Brazilian : Denmark :: X   |\n",
    "\n",
    "\n",
    "Try the analogies from the table. Is the correct answer returned for all queries? \n",
    "If not: are the answers at least ranked high?\n",
    "\n",
    "* a) Think of another category of *semantic* analogies that might be encoded in the embeddings and test this empirically by thinking of 5 example analogies. Which embeddings are better at predicting your category (Twitter versus Google News)?\n",
    "\n",
    "* b) Think of another category of *syntactic* analogies that might be encoded in the embeddings and test this empirically by thinking of 5 example analogies. Which embeddings are better at predicting your category (Twitter versus Google News)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('Dublin', 0.7911565899848938), ('London', 0.7881398797035217)], [('London', 0.4688378870487213), ('Twickenham', 0.45996522903442383)])\n",
      "([('she', 0.9052495360374451), ('she…', 0.782346785068512)], [('she', 0.7908554673194885), ('She', 0.6058069467544556)])\n",
      "([('mustard', 0.7218776941299438), ('green', 0.7193830609321594)], [('brown', 0.506827712059021), ('Bermuda_grass', 0.5015184283256531)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('better', 0.7841683626174927), ('worse', 0.7769781351089478)], [('better', 0.7236929535865784), ('worse', 0.5950594544410706)])\n",
      "([('drank', 0.8187708854675293), ('shotgunned', 0.7269829511642456)], [('drinks', 0.6172131299972534), ('drank', 0.5977991819381714)])\n",
      "([('Romanian', 0.8317359089851379), ('Bulgarian', 0.8283323645591736)], [('Danish', 0.8729315996170044), ('Swedish', 0.7290647625923157)])\n"
     ]
    }
   ],
   "source": [
    "def cosadd(vec1, vec2, vec3):\n",
    "    tw = twitEmbs.most_similar(positive=[vec2, vec3], negative=[vec1], topn=2)\n",
    "    go = googEmbs.most_similar(positive=[vec2, vec3], negative=[vec1], topn=2)\n",
    "    return tw, go \n",
    "print(cosadd('Denmark', 'Copenhagen', 'England'))\n",
    "print(cosadd('boy', 'girl', 'he'))\n",
    "print(cosadd('sky', 'blue', 'grass'))\n",
    "print(cosadd('nice', 'nicer', 'good'))\n",
    "print(cosadd('work', 'worked', 'drink'))\n",
    "print(cosadd('Brazil', 'Brazilian', 'Denmark'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('student', 0.7544766068458557), ('school', 0.7415605187416077)], [('elementary', 0.6052683591842651), ('school', 0.5762953758239746)])\n",
      "([('squeaks', 0.7607443928718567), ('bursts', 0.7025244235992432)], [('roar', 0.4860982298851013), ('meows', 0.4726291596889496)])\n",
      "([('screen', 0.6950099468231201), ('screen,', 0.692493736743927)], [('voiceover_narration', 0.4480317533016205), ('cameras', 0.4420016407966614)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('cockpit', 0.6811436414718628), ('sprinkler', 0.6798239350318909)], [('cockpit', 0.46297597885131836), ('airplane', 0.4398004710674286)])\n",
      "([('aurora', 0.6533317565917969), ('owl', 0.6438174843788147)], [('Atlantic_Ocean', 0.5046343207359314), ('Pacific_Ocean', 0.49234429001808167)])\n"
     ]
    }
   ],
   "source": [
    "# a) Semantic analogies\n",
    "\n",
    "print(cosadd('doctor', 'hospital', 'teacher'))\n",
    "print(cosadd('lion', 'roars', 'cat'))\n",
    "print(cosadd('painter', 'brush', 'writer'))\n",
    "print(cosadd('chef', 'kitchen', 'pilot'))\n",
    "print(cosadd('fish', 'ocean', 'bird'))\n",
    "\n",
    "# Google wins by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('jumped', 0.8450671434402466), ('flipped', 0.8145832419395447)], [('jumped', 0.7264297008514404), ('leaped', 0.6826007962226868)])\n",
      "([('slowed', 0.5967739224433899), ('idly', 0.595432698726654)], [('painfully_slow', 0.5080717206001282), ('glacial_pace', 0.5063737034797668)])\n",
      "([('geese', 0.6491351127624512), ('hares', 0.6470092535018921)], [('geese', 0.6305144429206848), ('Canada_geese', 0.5270898342132568)])\n",
      "([('teacher,', 0.6527166366577148), ('teaching', 0.6240153908729553)], [('teaches', 0.60946124792099), ('taught', 0.6061096787452698)])\n",
      "([('colder', 0.8040951490402222), ('warmer', 0.7245747447013855)], [('colder', 0.7085322737693787), ('warmer', 0.5902280807495117)])\n"
     ]
    }
   ],
   "source": [
    "# b) Syntactic analogies\n",
    "\n",
    "print(cosadd('walk', 'walked', 'jump'))\n",
    "print(cosadd('happy', 'happily', 'slow'))\n",
    "print(cosadd('mouse', 'mice', 'goose'))\n",
    "print(cosadd('swim', 'swimmer', 'teach')) # Google\n",
    "print(cosadd('quick', 'quicker', 'cold'))\n",
    "\n",
    "# Google wins by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Learning Word Embeddings\n",
    "\n",
    "\n",
    "So far you've learned about distributional semantics (vector semantics) in both the traditional and modern neural way, and you qualitatively worked with pre-trained (off-the-shelf) word embeddings in the last assignment.\n",
    "\n",
    "In this assignment, you will learn how to implement a neural network  to learn word embeddings, namely the *Continous Bag of Words* (CBOW) model for word embeddings. More specifically, you will:\n",
    "\n",
    "* learn how to represent text for windows language modeling\n",
    "* learn how to design a Pytorch model (`torch.module`)\n",
    "* learn how to implement a FNN for learning embeddings with CBOW which *sums* the context embedding vectors\n",
    "* train the model for a few epochs using stochastic gradient descent (SGD)\n",
    "* read off the learned embeddings $W$, store them in a gensim-readable file and inspect them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "\n",
    "\n",
    "\n",
    "CBOW is a model proposed by [Mikolov et al., 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n",
    "\n",
    "It is a simple neural method to learn word embeddings and it is one of the two core algorithms in the `word2vec` toolkit (see figure below). Note that, besides its usage here to learn word embeddings, CBOW is also a more general term used to refer to any input representation which consists of (some) way of aggregating a set of word embeddings. Hence its name, the continous BOW representation. You can in fact use such a similar representation (e.g., the average of the embeddings of words) for other tasks as well, such as text classification. Here, CBOW is meant in its original formulation: a network over the *sum* of embeddings of context words aimed at predicting the middle target word. It is related in spirit to a language model, but instead framed as a classification task (with context available on both sides) and hence bears more similarities with a *[word close test](https://en.wikipedia.org/wiki/Cloze_test)*.\n",
    "\n",
    "Illustration of the CBOW model (in comparison to the skip-gram):\n",
    "<img src=\"pics/cbow-vs-skipgram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bonus 6. Representing the data\n",
    "\n",
    "Given a corpus, extract the training data for the CBOW model using a window size of 2 words on each side of the target word. The following image shows what the input of the training algorithm (`Input`) should look like (`Training window`):\n",
    "\n",
    "\n",
    "<img src=\"pics/cbow-window.jpg\">\n",
    "\n",
    "Hints:\n",
    "* Remember to `\"<pad>\"` the input when the window size is smaller than the expected window size. This also means that the `\"<pad>\"` token should be in the vocabulary; reserve the first `0` index for this special token.\n",
    "* In Pytorch, all input is expected to be a `torch.tensor`. You can create these beforehand with `torch.zeros()`, or just convert a resulting python list by using `torch.tensor(train_data)`.\n",
    "\n",
    "Example:\n",
    "\n",
    "Given the following tiny corpus:\n",
    "```\n",
    "tiny_corpus = [\"this is an example\", \"this is a longer example sentence\", \"I love deep learning\"]\n",
    "```\n",
    "\n",
    "To create the `train_X` data, you first need to extract n-gram windows and the target words:\n",
    "\n",
    "```\n",
    "label,context\n",
    "this ['<pad>', '<pad>', 'is', 'an']\n",
    "is ['<pad>', 'this', 'an', 'example']\n",
    "example ['this', 'is', 'example', '<pad>']\n",
    "...\n",
    "```\n",
    "\n",
    "And convert them into numeric format, where each word token is represented by its unique index:\n",
    "\n",
    "```\n",
    "train_labels = [ 1,  2,  3,  4,  1,  2,  5,  6,  4,  7,  8,  9, 10, 11]\n",
    "train_data = [[ 0,  0,  2,  3],\n",
    " [ 0,  1,  3,  4],\n",
    " [ 1,  2,  4,  0],\n",
    " [ 2,  3,  0,  0],\n",
    " [ 0,  0,  2,  5],\n",
    " [ 0,  1,  5,  6],\n",
    " [ 1,  2,  6,  4],\n",
    " [ 2,  5,  4,  7],\n",
    " [ 5,  6,  7,  0],\n",
    " [ 6,  4,  0,  0],\n",
    " [ 0,  0,  9, 10],\n",
    " [ 0,  8, 10, 11],\n",
    " [ 8,  9, 11,  0],\n",
    " [ 9, 10,  0,  0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_corpus = [\"this is an example\", \"this is a longer example sentence\", \"I love deep learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_corpus = []\n",
    "with open('sample.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        tiny_corpus.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestion: Implement all your steps first on the `tiny_corpus` data. Then test your implementation on the provided data `sample.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0,   0,   2,   3],\n",
       "         [  0,   1,   3,   4],\n",
       "         [  1,   2,   4,   2],\n",
       "         ...,\n",
       "         [ 11,  43,   2, 840],\n",
       "         [ 43, 817, 840,   0],\n",
       "         [817,   2,   0,   0]]),\n",
       " tensor([  1,   2,   3,  ..., 817,   2, 840]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## global settings\n",
    "PAD = \"<PAD>\"\n",
    "window_size=2\n",
    "\n",
    "### your code here\n",
    "def generate_ngrams(sentence, n):\n",
    "    tokens = [PAD]*n + sentence.split() + [PAD]*n\n",
    "    ngrams = [([tokens[i-j-1] for j in reversed(range(n))] + [tokens[i+j+1] for j in range(n)], tokens[i])\n",
    "              for i in range(n, len(tokens)-n)]\n",
    "    return ngrams\n",
    "\n",
    "# Tokenize the sentences and create a vocabulary\n",
    "tokens = [word for sentence in tiny_corpus for word in sentence.split()]\n",
    "\n",
    "word2idx = {PAD: 0}\n",
    "\n",
    "# Assign indices starting from 1 in the order they appear in the corpus\n",
    "for word in tokens:\n",
    "    if word not in word2idx:\n",
    "        word2idx[word] = len(word2idx)\n",
    "\n",
    "ngrams = [generate_ngrams(sentence, window_size) for sentence in tiny_corpus]\n",
    "ngrams = [item for sublist in ngrams for item in sublist]  # Flatten the list of n-grams\n",
    "\n",
    "# Convert words to their numeric indices using the mapping\n",
    "train_data = torch.tensor([[word2idx[word] for word in context] for context, _ in ngrams])\n",
    "train_labels = torch.tensor([word2idx[target] for _, target in ngrams])\n",
    "\n",
    "# Results\n",
    "train_data, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bonus 7. Implement the continuous bag of words model for estimating word embeddings\n",
    "\n",
    "Implement the CBOW model for word embeddings: a CBOW with window size 2, which `sums` the input embeddings and from that hidden representation `predicts` the target token. \n",
    "\n",
    "The steps for CBOW are as follows:\n",
    "* Convert your data to the center/window (done in previous assignment\n",
    "* The model should have an embedding layer a linear layer (and optionally a loss function, you can also put the loss function in the forward loop)\n",
    "* In the forward function of the model, it should: look up the embeddings, sum them, convert to logits (in the linear layer), and optionally calculate the loss (can also be done in forward loop)\n",
    "* In the training loop (assignment 4b), we have a for loop for the epochs and one for the data. Within this, we call the forward function and obtain the loss after which the backward pass can be called\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model in Pytorch, one has to define a sub-class of `torch.nn.module` (see also [assignment3](https://github.itu.dk/robv/intro-nlp2023/blob/main/assignments/week3/train.py)). The constructor `__init__()` and the `forward()` function can then be defined to specify the structure of the network. In the `__init__` function, the layers are specified and initialized, whereas the `forward` function defines how the layers interact during a forward-pass. You can use [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) for the embedding layer, [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) for the hidden layer, and [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) as loss function. \n",
    "\n",
    "For some examples we refer to this [tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) and this [introduction](https://towardsdatascience.com/an-easy-introduction-to-pytorch-for-neural-networks-3ea08516bff2).\n",
    "\n",
    "* a) Implement the CBOW network as described above:\n",
    "\n",
    "**Hint**: you can print the structure of the model by simply printing the initialized variable. Make sure all the layers are represented in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (embeddings): Embedding(14532, 64)\n",
      "  (linear): Linear(in_features=64, out_features=14532, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "embed_dim = 64\n",
    "vocab_dim = len(word2idx)\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_dim, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_sum = embeds.sum(dim=0).view(1, -1) # Sum and reshape embeddings\n",
    "        logits = self.linear(embeds_sum)\n",
    "        return logits\n",
    "\n",
    "cbow_model = CBOW(embed_dim,vocab_dim)\n",
    "print(cbow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) Now implement the training procedure with gradient descent (`learning rate=0.001`). Go through the dataset `10` times, and update the weights after each line (`batch size = 1`). An example of a training procedure can be found on: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network\n",
    "\n",
    "**Hint**: you have to convert the lists created in assignment 3 to be able to do the forward pass. The forward pass expects its input to be in tensors. So for the gold labels this means we have to ensure that we do not pass a zero-dimension tensor which looks like: `tensor(1)`, but convert this to `tensor([1])`. Similarly for the training data, we convert `tensor([0, 0, 2, 3])` to `tensor([[0], [0], [2], [3]])`. This can be done with [tensor views](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [07:21<1:06:13, 441.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 795465.8421205501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [14:07<56:04, 420.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 703642.9686597846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [20:49<48:05, 412.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 659211.8396050427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [27:36<41:00, 410.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 629916.4330505263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [33:45<32:55, 395.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 607484.8932853411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [40:05<26:01, 390.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 589339.6070791595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [46:23<19:18, 386.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 574111.0862202514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [53:14<13:07, 393.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 560905.0768480739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [59:50<06:34, 394.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 549154.8300352106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:06:14<00:00, 397.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 538506.09074019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "from tqdm import tqdm\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cbow_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    total_loss = 0\n",
    "    for context, target in zip(train_data, train_labels):\n",
    "        # Prepare data\n",
    "        context_var = context.view(-1, 1)\n",
    "\n",
    "        # Reset gradients before each iteration\n",
    "        cbow_model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        log_probs = cbow_model(context_var)\n",
    "\n",
    "        # Compute the loss, gradients, and update the parameters\n",
    "        loss = loss_function(log_probs, target.view(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
